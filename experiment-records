前几次试验结果没有形成日志文档，暂且忽略。从20171017开始记录。
1———————————————xgboost———0.269————————————————————————————————————
--下采样平衡数据
 (1) 得分值：
    本地交叉验证集> score:[train:0.353957+0.00145077, valid:0.269652+0.00496595]
    本地测试集 >   score:[train:0.342468, test:0.268326]
 (2)数据处理
    a. 将所有分类变量进行OHE转换；
    b. 删掉训练集中取值唯一（只有一个值）的列；
    c. 其他不做任何处理（缺失值保持默认缺失标志）
    d. 将整个训练集train_origin按照8:2的比例以分层抽样的形式随机划分为训练集和测试集train\test;
    e. 从train中对负类样本利用简单随机抽样方法进行下采样，采样数为正类（少数类）的样本量，进行样本平衡，得到一个平衡的训练集train_sample;
 (3)模型训练
   直接利用xgboost分类算法训练模型。
   
2————————————————LightGBM——0.275————————————————————————————————————————————-
--原始数据，未做任何特征处理
（1）得分值(auc)：
    本地交叉验证集> score:[train:0.683399+0.001286 ,valid:0.626162+0.002588]
    本地测试集>    score:[0.6381]
 (2)数据未做任何处理
 
3———————————————LightGBM——0.274———————————————————————————————————————
--基于试验2模型给出的特征评分，剔除得分排在末位的部分变量，其他数据处理与试验2一致
得分值（auc）:
    本地交叉验证集> score [train:0.640937+0.000369 ,valid:0.637589+0.002706]
    本地测试集> score[0.6369]
4———————————————LightGBM———0.279—————————————————————————————————————
 --分类变量哑编码，剔除方差为零的变量，保持原始数据不平衡比例
  得分值(auc)：
    本地交叉验证> score:[train:0.640937+0.000369, valid:0.67268+0.002706]
    本地测试集 > score:[0.6413]
5———————————————LightGBM+RandomTreesEmbedding——————0.281—————————————————————————————————————————————
--利用RandomTreesEmbedding做无监督特征转换构造新特征，剔除calc大类变量，分类变量哑编码，剔除方差为零的变量，保持原始数据比例
--原始文件20171107_18LightGBM_dummy_RandomTreeEmbedding_del_calc
得分：
   本地交叉验证> score:[train:0.696729+0.000703, valid:0.642507+0.002792]
   本地测试集 > score:[0.6429]
6——————————————LightGBM————————————0.279——————————————————————————————————————————————————————————————————
--剔除calc大类变量，分类变量哑编码，剔除方差为零的变量，保持原始数据比例
--原始文件：20171107_19LightGBM_dummy_del_calc
得分：
本地交叉验证：
0.642025          0.688985        0.003235          0.00083
 测试集：0.6423

阶段总结：
（1）分类变量做哑编码是有效果的，提升模型性能；
（2）剔除所有calc组内变量提升模型性能；
（3）利用RandomTreesEmbedding做特征转换生成新特征，加入原始特征一起会提升模型性能；
